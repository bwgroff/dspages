{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer LTV\n",
    "- categories: [Julia, Turing, Churn, Survival, LTV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "using Turing\n",
    "using Gadfly\n",
    "using DataFrames, DataFramesMeta\n",
    "Gadfly.set_default_plot_size(900px, 300px)\n",
    "ENV[\"COLUMNS\"] = 120;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Lifetime Value (LTV or CLTV) is the total dollar value a consumer will spend at a business throughout their life. The concept is as important as the definition is straightforward - businesses very often want to know which consumers are their whales and which are eating up their marketing or infrastructure budgets with little or no value returned. This is pretty tricky and there are a few approaches you can take:\n",
    "\n",
    "### Observational\n",
    "\n",
    "**Naive calculation**. The following will give you an average that is delightfully simple but tragically wrong:\n",
    "\n",
    "$$\\mathrm{LTV} = \\frac{1}{|\\mathrm{Customers}|}\\sum_{\\mathrm{orders}} \\mathrm{Order\\ Value}$$\n",
    "\n",
    "Assuming (hmm) that LTV is constant over time, this will converge to the true average LTV value as customers churn (and thus achieve their final lifetime value). New customers will continue to weigh the average down and make it an underestimate. There are some of these sort of equations floating around the tubes.\n",
    "\n",
    "**Wait and see**. Simialr algorithm to the above, the major difference is applying this to only a small cohort from a brief window in time. Just follow along with that group and add up how much they spend. This is simple and will get to the true LTV of that cohort faster but it's still typically too slow to be useful. By the time you know, it's months/quarters/years later (depending on the churn / repurchasing characteristics of your product) and most insights you might glean are no longer relevant to your product roadmap.\n",
    "\n",
    "### Modeled\n",
    "\n",
    "**Machine Learning** :tada:. There are a bunch of ML approaches that can be found relatively easily online (but apparently not easy enough for me to find them again to include here). IIRC, one was using a random forest (or GBM, or whatever) to predict \n",
    "\n",
    "$$P(\\mathrm{purchase\\ in\\ next\\ period}|\\mathcal{D})$$ \n",
    "\n",
    "and then in a second stage model (conditioned on the purchase outcome) predict the order value of said purchase.\n",
    "\n",
    "It's a reasonably standard approach: decompose the problem into churn, expected future purchases, and expected value per purchase. There are a bunch of approaches that are tailored to this decomposition by breaking down the inputs into the so called **RFM** metrics: \n",
    "\n",
    "- **R**ecency: time since the last purchase,\n",
    "- **F**requency: number of purchases per time period, \n",
    "- **M**onetary value: average order value.\n",
    "\n",
    "Note that we'll use days for the time scale.\n",
    "\n",
    "**Buy 'til You Die**. This type of model was popularized by Schmittlein, David C., Donald G. Morrison, and Richard Colombo in 1987 but was apparently not very easy to implement. A simpler version was created by [Fader, Hardie and Lee](http://www.brucehardie.com/papers/bgnbd_2004-04-20.pdf) and Fader at least built a company that expanded quite a bit on these sorts of models, [Zodiac](https://www.zdnet.com/article/nikes-purchase-of-analytics-firm-zodiac-highlights-focus-on-customer-lifetime-value/).\n",
    "\n",
    "**Custom Model**. That's what we're going to do! Fader and Hardie do a great job of making their work look harder than necessary so I can't be bothered to decode it (and anyway, [Alex did a great job](https://medium.com/ordergroove-engineering/every-customer-counts-52aa70e4f85)). That said, I'm going to take what seems to be a similar approach and takes advantage of some more modern techniques (Julia and Turing.jl!):\n",
    "\n",
    "1. Estimate churn based on **R**ecency and **F**requency.\n",
    "2. Set up a super simple survival model to understand the expected number of future purchases using sample from (1) as the churn signal.\n",
    "3. Scale by **M**onetary value.\n",
    "\n",
    "By building these submodels out independently we can understood the whole model by figuring it out component-by-component. It also provides a quick way to make single-component adjustments that might be important. There will be many, this model has some real obvious deficiencies even though it captures the right ideas.\n",
    "\n",
    "For instance, some retailers have an extremely wide spread of possible order values (e.g. Walmart, you can buy a stick of gum or probably a boat or something). If there are orders-of-magnitude differences in purchase value then you better model that out so you know exactly which consumers are likely to find themselves in that lucrative long tail. In my experience, lognormal is a decent start but the tail is still too light."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Models\n",
    "\n",
    "### Active from RF\n",
    "\n",
    "We sample when we expect the customer's next purchase to occur based on what we've observed of their frequency, then we compare that to how long it's been since they purchased. If we expected them to have purchased already but they haven't then we count them as churned.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{next\\ purchase} &\\sim \\mathrm{Exponential}(F) \\\\\n",
    "\\mathrm{active} &= R < \\mathrm{next\\ purchase}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that we don't have any kind of regularization and just assume F is a fine number for us. Exercise for the reader to make that more stable :smile:. And also, what should we do about customers with only 1 purchase? :scream:\n",
    "\n",
    "### Future Purchases from RF+Active\n",
    "\n",
    "We'd like to then take the inferences above and use them to understand churn as a function of time, or perhaps number of orders. In other words:\n",
    "\n",
    "$$P(\\mathrm{churned}_{t=i} | \\mathrm{active}_{t=i-1})$$\n",
    "\n",
    "Here we find some wrinkles. Most notably, what to do with consumers that have recently purchased and we don't know if they are going to churn before the next purchase? This is called censoring, which comes in many directional varieties and this variety is called right-censoring (on the \"right\" side of our time interval, we don't yet have data on the outcome). We'll ignore that for now, and instead assume \"constant hazard\" on the data we can observe, ie the rate at which users remain active ($\\rho$) is constant across all time points.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho &\\sim \\mathrm{Beta}(1,1)\\\\\n",
    "\\mathrm{purchases}_{uncensored} &\\sim \\mathrm{Geometric(\\rho)}\\\\\n",
    "(\\mathrm{Future\\ purchases}) &\\sim \n",
    "    \\begin{cases}\n",
    "    \\mathrm{Geometric}(\\rho) & \\mathrm{if\\ active} \\\\\n",
    "    \\mathrm{Dirac}(0) & \\mathrm{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### LTV from M+Future Purchases\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Future\\ value} &= \\mathrm{Future\\ purchases} * \\mathrm{AOV}\\\\\n",
    "\\mathrm{Lifetime\\ value} &= \\mathrm{Future\\ value} + \\mathrm{Past\\ value}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up some Daaaataaa\n",
    "\n",
    "A little toy dataset to see if the Active model makes any kind of sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CustomerData\n",
    "    days_since_last_purchase::Int64\n",
    "    days_since_first_purchase::Int64\n",
    "    total_purchases::Int64\n",
    "    monetary_value::Float64\n",
    "end\n",
    "\n",
    "struct RFM\n",
    "    raw::CustomerData\n",
    "    recency::Int64\n",
    "    frequency::Float64\n",
    "    monetary_value::Float64\n",
    "    total_purchases::Int64\n",
    "end\n",
    "\n",
    "function rfm(c::CustomerData)\n",
    "    period = (c.days_since_first_purchase - c.days_since_last_purchase) / (c.total_purchases - 1)\n",
    "    rfm = RFM(\n",
    "        c,\n",
    "        c.days_since_last_purchase,\n",
    "        1 / period,\n",
    "        c.monetary_value,\n",
    "        c.total_purchases)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_data = [rfm(c) for c in [\n",
    "    CustomerData(2,   60,  5,   3),   \n",
    "    CustomerData(10, 305, 10,  23),\n",
    "    CustomerData(53, 100, 40, 123),    # definitely churned!\n",
    "    CustomerData(2,   29,  3, 123),\n",
    "    CustomerData(10, 200,  5,  23),\n",
    "    CustomerData(23, 222, 20,   3),    # probably churned..\n",
    "]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function active(custs::Array{RFM})\n",
    "    predicted_purchase_days = Vector(undef, length(custs))\n",
    "    active = Vector{Bool}(undef, length(custs))\n",
    "\n",
    "    for i in 1:length(custs)\n",
    "        predicted_purchase_days[i] ~ Exponential(1.0 / custs[i].frequency) \n",
    "        active[i] = predicted_purchase_days[i] > custs[i].recency\n",
    "    end\n",
    "    \n",
    "    return active\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "iterations = 1000\n",
    "ϵ = 0.05\n",
    "τ = 10;\n",
    "\n",
    "chain_ltv = sample(\n",
    "    active(rfm_data), \n",
    "    HMC(ϵ, τ), iterations, \n",
    "    progress=false, drop_warmup=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model's output matches up with our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1_mean</th><th>x2_mean</th><th>x3_mean</th><th>x4_mean</th><th>x5_mean</th><th>x6_mean</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>1 rows × 6 columns</p><tr><th>1</th><td>0.891</td><td>0.675</td><td>0.0</td><td>0.943</td><td>0.779</td><td>0.111</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& x1\\_mean & x2\\_mean & x3\\_mean & x4\\_mean & x5\\_mean & x6\\_mean\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.891 & 0.675 & 0.0 & 0.943 & 0.779 & 0.111 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x1_mean \u001b[0m\u001b[1m x2_mean \u001b[0m\u001b[1m x3_mean \u001b[0m\u001b[1m x4_mean \u001b[0m\u001b[1m x5_mean \u001b[0m\u001b[1m x6_mean \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────\n",
       "   1 │   0.891    0.675      0.0    0.943    0.779    0.111"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_samples = DataFrame(hcat(generated_quantities(active(rfm_data), chain_ltv)...)')\n",
    "combine(active_samples, :x1 => mean, :x2 => mean, :x3 => mean, :x4 => mean, :x5 => mean, :x6 => mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `x1_mean` is the probability that the first customer is still active. These numbers look pretty reasonable to me, even though we didn't account for any uncertainty around F (or, like, what to do with customers that only purchased 1 time... alas).\n",
    "\n",
    "Notice that I used `generated_quantities` here. This is possible because we have `return active` in the model block. The Turing handling of generated quantities is... just ok, sort of awkward to work with. :grimacing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And with the CDNow dataset...\n",
    "\n",
    "The raw data can be found [here](https://www.brucehardie.com/datasets/) and represents a cohort of users that made their first purchase at CDNow in Q1 of 1997."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>customer</th><th>date</th><th>count</th><th>usd</th></tr><tr><th></th><th>Int64</th><th>Date…</th><th>Int64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 4 columns</p><tr><th>1</th><td>1</td><td>1997-01-01</td><td>1</td><td>11.77</td></tr><tr><th>2</th><td>2</td><td>1997-01-12</td><td>1</td><td>12.0</td></tr><tr><th>3</th><td>2</td><td>1997-01-12</td><td>5</td><td>77.0</td></tr><tr><th>4</th><td>3</td><td>1997-01-02</td><td>2</td><td>20.76</td></tr><tr><th>5</th><td>3</td><td>1997-03-30</td><td>2</td><td>20.76</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& customer & date & count & usd\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Date… & Int64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 1997-01-01 & 1 & 11.77 \\\\\n",
       "\t2 & 2 & 1997-01-12 & 1 & 12.0 \\\\\n",
       "\t3 & 2 & 1997-01-12 & 5 & 77.0 \\\\\n",
       "\t4 & 3 & 1997-01-02 & 2 & 20.76 \\\\\n",
       "\t5 & 3 & 1997-03-30 & 2 & 20.76 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m customer \u001b[0m\u001b[1m date       \u001b[0m\u001b[1m count \u001b[0m\u001b[1m usd     \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Date…      \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────────────────\n",
       "   1 │        1  1997-01-01      1    11.77\n",
       "   2 │        2  1997-01-12      1    12.0\n",
       "   3 │        2  1997-01-12      5    77.0\n",
       "   4 │        3  1997-01-02      2    20.76\n",
       "   5 │        3  1997-03-30      2    20.76"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse\n",
    "using CSV\n",
    "cdnow = CSV.read(\"/Users/brad/data/cleaned_cdnow.csv\", DataFrame) # oh no now you know where my filez\n",
    "first(cdnow, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get it a little closer to RFM format, which gives us the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>customer</th><th>total_purchases</th><th>first_purchase_dt</th><th>latest_purchase_dt</th><th>monetary_value</th><th>days_since_first_purchase</th><th>days_since_last_purchase</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Date</th><th>Date</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>1</td><td>1</td><td>1997-01-01</td><td>1997-01-01</td><td>11.77</td><td>90</td><td>90</td></tr><tr><th>2</th><td>2</td><td>2</td><td>1997-01-12</td><td>1997-01-12</td><td>89.0</td><td>79</td><td>79</td></tr><tr><th>3</th><td>3</td><td>2</td><td>1997-01-02</td><td>1997-03-30</td><td>41.52</td><td>89</td><td>2</td></tr><tr><th>4</th><td>4</td><td>2</td><td>1997-01-01</td><td>1997-01-18</td><td>59.06</td><td>90</td><td>73</td></tr><tr><th>5</th><td>5</td><td>3</td><td>1997-01-01</td><td>1997-02-04</td><td>82.2</td><td>90</td><td>56</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& customer & total\\_purchases & first\\_purchase\\_dt & latest\\_purchase\\_dt & monetary\\_value & days\\_since\\_first\\_purchase & days\\_since\\_last\\_purchase\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Date & Date & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 1 & 1997-01-01 & 1997-01-01 & 11.77 & 90 & 90 \\\\\n",
       "\t2 & 2 & 2 & 1997-01-12 & 1997-01-12 & 89.0 & 79 & 79 \\\\\n",
       "\t3 & 3 & 2 & 1997-01-02 & 1997-03-30 & 41.52 & 89 & 2 \\\\\n",
       "\t4 & 4 & 2 & 1997-01-01 & 1997-01-18 & 59.06 & 90 & 73 \\\\\n",
       "\t5 & 5 & 3 & 1997-01-01 & 1997-02-04 & 82.2 & 90 & 56 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m customer \u001b[0m\u001b[1m total_purchases \u001b[0m\u001b[1m first_purchase_dt \u001b[0m\u001b[1m latest_purchase_dt \u001b[0m\u001b[1m monetary_value \u001b[0m\u001b[1m days_since_first_purchase \u001b[0m\u001b[1m da\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Int64           \u001b[0m\u001b[90m Date              \u001b[0m\u001b[90m Date               \u001b[0m\u001b[90m Float64        \u001b[0m\u001b[90m Int64                     \u001b[0m\u001b[90m In\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "   1 │        1                1  1997-01-01         1997-01-01                   11.77                         90     ⋯\n",
       "   2 │        2                2  1997-01-12         1997-01-12                   89.0                          79\n",
       "   3 │        3                2  1997-01-02         1997-03-30                   41.52                         89\n",
       "   4 │        4                2  1997-01-01         1997-01-18                   59.06                         90\n",
       "   5 │        5                3  1997-01-01         1997-02-04                   82.2                          90     ⋯\n",
       "\u001b[31m                                                                                                        1 column omitted\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse\n",
    "using Dates\n",
    "\n",
    "cutoff_date = Date(\"1997-04-01\")\n",
    "\n",
    "cdnow_gdf = @linq cdnow |>\n",
    "    where(:date .< cutoff_date) |>\n",
    "    groupby(:customer) \n",
    "\n",
    "pre_rfm = combine(cdnow_gdf, \n",
    "    nrow  => :total_purchases, \n",
    "    :date => minimum => :first_purchase_dt,\n",
    "    :date => maximum => :latest_purchase_dt,\n",
    "    :usd  => sum     => :monetary_value)\n",
    "\n",
    "function days_val(days)\n",
    "    return days.value\n",
    "end\n",
    "\n",
    "rfm_df = @linq pre_rfm |> \n",
    "    transform(\n",
    "        days_since_first_purchase = days_val.(cutoff_date - :first_purchase_dt),\n",
    "        days_since_last_purchase  = days_val.(cutoff_date - :latest_purchase_dt)\n",
    "    )\n",
    "\n",
    "first(rfm_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can blast into RFM format. The model requires some tiny adjustments because in this dataset we have:\n",
    "- Customers with only one purchase,\n",
    "- Customers with only one purchase date but multiple purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "rfm_cdn = [\n",
    "    rfm(\n",
    "        CustomerData(\n",
    "            row.days_since_last_purchase,\n",
    "            row.days_since_first_purchase,\n",
    "            row.total_purchases,\n",
    "            row.monetary_value)\n",
    "    ) for row in eachrow(rfm_df)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function active_cdn(custs::Array{RFM})\n",
    "    predicted_purchase_days = Vector(undef, length(custs))\n",
    "    active = Vector{Bool}(undef, length(custs))\n",
    "\n",
    "    for i in 1:length(custs)\n",
    "        if isnan(custs[i].frequency) | isinf(custs[i].frequency)\n",
    "            predicted_purchase_days[i] ~ Exponential(15.0) # median period for multiple purchasers \n",
    "        else\n",
    "            predicted_purchase_days[i] ~ Exponential(1.0 / custs[i].frequency) \n",
    "        end\n",
    "        active[i] = predicted_purchase_days[i] > custs[i].recency\n",
    "    end\n",
    "    \n",
    "    return active\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:24\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "chain_ltv_cdn = sample(\n",
    "    active_cdn(rfm_cdn[1:100]), \n",
    "    HMC(ϵ, τ), iterations, \n",
    "    progress=true, drop_warmup=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yeah, it's on the slow side.. well not slow given how much cool stuff is happening. There are like 20k customers in this dataset and only the first 100 took 30s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Purchases and LTV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function future_purchases_cdn(custs::Array{RFM}, total_purchases::Array{Float64})\n",
    "\n",
    "    # Active submodel\n",
    "    predicted_purchase_days = Vector(undef, length(custs))\n",
    "    active = Vector{Bool}(undef, length(custs))\n",
    "\n",
    "    for i in 1:length(custs)\n",
    "        if isnan(custs[i].frequency) | isinf(custs[i].frequency)\n",
    "            predicted_purchase_days[i] ~ Exponential(15.0) # median period for multiple purchasers \n",
    "        else\n",
    "            predicted_purchase_days[i] ~ Exponential(1.0 / custs[i].frequency) \n",
    "        end\n",
    "        active[i] = predicted_purchase_days[i] > custs[i].recency\n",
    "    end\n",
    "    \n",
    "    # Future purchases submodel\n",
    "    churn_rate ~ Beta(1,1)\n",
    "    future_purchases = Vector(undef, length(custs))\n",
    "    for i in 1:length(custs)\n",
    "        if !active[i]\n",
    "            total_purchases[i] ~ Exponential(churn_rate)\n",
    "            future_purchases[i] ~ Exponential(1e-3)\n",
    "        else\n",
    "            future_purchases[i] ~ Exponential(churn_rate)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # LTV \"model\"\n",
    "    ltv = future_purchases .* [c.monetary_value for c in custs]\n",
    "    return active, ltv\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:02:17\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "total_purchases = [float(r.total_purchases) for r in rfm_cdn]\n",
    "\n",
    "chain_future_purchases = sample(\n",
    "    future_purchases_cdn(rfm_cdn[1:100], total_purchases[1:100]), \n",
    "    HMC(ϵ, τ), iterations, \n",
    "    progress=true, drop_warmup=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's the model! Unfortunately, wrangling `generated_quantities` can be annoying so making nice plots will have to wait (more to come!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
