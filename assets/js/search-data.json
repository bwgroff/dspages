{
  
    
        "post0": {
            "title": "Coinflipping Zoo",
            "content": "Coinflipping is a time honored example for probability exercises, and for good reason. It is readily interpretable while providing ample opportunity for more complex scenarios. Nevermind that &quot;unfair&quot; coins are essentially impossible to make... In any case, a few examples of coin flipping! . using Turing using Gadfly using DataFrames, DataFramesMeta . . Scenario: Trivial (Un)fair Coin . The simplest possible example: imagine we have a coin that might (or might not!) be fair. blah blah . @model function coinflip(y) # prior on p p ~ Beta(1, 1) # updates on p for i in 1:length(y) y[i] ~ Bernoulli(p) end end . coinflip (generic function with 1 method) . data = [1,1,1,0,0,1,0,0,1,1,1,1,1,0,0,0] # Settings of the Hamiltonian Monte Carlo (HMC) sampler. iterations = 1000 ϵ = 0.05 τ = 10 . chain_bernoulli = sample(coinflip(data), HMC(ϵ, τ), iterations, progress=false); chain_bernoulli . Chains MCMC chain (1000×10×1 Array{Float64,3}): Iterations = 1:1000 Thinning interval = 1 Chains = 1 Samples per chain = 1000 parameters = p internals = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, n_steps, nom_step_size, step_size Summary Statistics parameters mean std naive_se mcse ess rhat Symbol Float64 Float64 Float64 Float64 Float64 Float64 p 0.5581 0.1148 0.0036 0.0048 373.2283 1.0029 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 p 0.3229 0.4873 0.5577 0.6392 0.7687 . plot(DataFrame(chain_bernoulli), x=:p, Geom.histogram) . . p -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 -25 0 25 50 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 Just to be clear, there are equivalent models that express the exact same underlying phenomena. Here&#39;s one example using Binomial in place of Bernoulli: . @model function coinflip_binomial(y) # prior on p p ~ Beta(1, 1) # updates on p heads = sum(y) heads ~ Binomial(length(y), p) end . coinflip_binomial (generic function with 1 method) . chain_binomial = sample(coinflip(data), HMC(ϵ, τ), iterations, progress=false); chain_binomial . Chains MCMC chain (1000×10×1 Array{Float64,3}): Iterations = 1:1000 Thinning interval = 1 Chains = 1 Samples per chain = 1000 parameters = p internals = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, n_steps, nom_step_size, step_size Summary Statistics parameters mean std naive_se mcse ess rhat Symbol Float64 Float64 Float64 Float64 Float64 Float64 p 0.5655 0.1115 0.0035 0.0063 365.6428 1.0009 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 p 0.3448 0.4896 0.5702 0.6439 0.7724 . plot(DataFrame(chain_binomial), x=:p, Geom.histogram) . . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 -25 0 25 50 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 Scenario: Autocorrelation . Neato! So, the simplest case is pretty easy to work with. What about (moving past coin flipping to generic binary data) we have some data that has a lot of streaks (1, 1, 1, 1, 1, 1, ...)? Our estimates of p and our error terms will be wildly skewed (downward!) if we assume the data has independence. If for instance the data instead has autocorrelation (flipping a 1 is more likely if the preceeding flip was also a 1, and the reverse) then our estimates will just be wrong unless we account for it explicitly. . Let&#39;s generate some data in a way that very explicitly shows the connection to the previous datapoint: . data_autocor2 = [1] p_1 = 0.85 p_2 = 0.15 for i in 2:1000 if data_autocor2[i-1] == 1 p = p_1 else p = p_2 end data_autocor2 = cat(data_autocor2, rand(Bernoulli(p)), dims=1) end . And here&#39;s a model - it&#39;s not an accident that the model code looks so close to the data generating code! . @model function coinflip_autocor(y) # prior on p p_1 ~ Beta(1, 1) p_2 ~ Beta(1, 1) y[1] ~ Bernoulli(p_1) for i in 2:length(y) if y[i-1] == 1 p = p_1 else p = p_2 end y[i] ~ Bernoulli(p) end end . coinflip_autocor (generic function with 1 method) . chain_autocor = sample(coinflip_autocor(data_autocor2), HMC(ϵ, τ), iterations, progress=false); chain_autocor . . Chains MCMC chain (1000×11×1 Array{Float64,3}): Iterations = 1:1000 Thinning interval = 1 Chains = 1 Samples per chain = 1000 parameters = p_1, p_2 internals = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, n_steps, nom_step_size, step_size Summary Statistics parameters mean std naive_se mcse ess rhat Symbol Float64 Float64 Float64 Float64 Float64 Float64 p_1 0.8905 0.0231 0.0007 0.0002 4898.2965 0.9990 p_2 0.1813 0.0297 0.0009 0.0004 4084.1890 0.9995 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 p_1 0.8640 0.8829 0.8913 0.8994 0.9159 p_2 0.1349 0.1661 0.1803 0.1972 0.2265 . plot(DataFrame(chain), layer(x=:p_1, Geom.histogram), layer(x=:p_2, Geom.histogram)) . . p_1 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -100 -80 -60 -40 -20 0 20 40 60 80 100 120 140 160 180 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 -100 0 100 200 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 How to do this with an honest covariance matrix? What about a &quot;rich get richer&quot; scenario? P(1) ~ b + length of streak of 1s . would look good but how to diagnose issues? distributions of streaks! a lot of extremely long streaks, a lot of short streaks... | . | . what about a &quot;machine error&quot; - double readings for both or one of 1s / 0s? . Scenario: Rich get Richer . Look, we&#39;re long passed the physical analogy making sense for this data so just roll with it for now. The idea for this scenario is a minor extension of the former: every consecutive flip heads makes it more likely the next flip will also be heads, and the reverse for tails. . function streaks(arr) streaks = [] val = arr[1] current_streak = 1 for i in 2:length(arr) next_val = arr[i] if next_val == val current_streak = current_streak + 1 else streaks = cat(streaks, current_streak, dims=1) val = next_val current_streak = 1 end end streaks = cat(streaks, current_streak, dims=1) return streaks end . streaks (generic function with 1 method) . streaks([1,1,1,1,2,2,2,2,3,2,1,2,2,2,1]) . 7-element Array{Any,1}: 4 4 1 1 1 3 1 . plot(DataFrame(x=streaks(data_autocor2)), x=:x, Geom.histogram) . x -100 -80 -60 -40 -20 0 20 40 60 80 100 120 140 160 180 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 -100 0 100 200 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 -25 0 25 50 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 Scenario: Many Coins of Few Types . &quot;data on several coins from specific mints. Coin of unknown origin with 10 flips, what is P(heads) on next flip?&quot; . Scenario: Coins with Continuous Bias . All coins come from one rather shitty mint. How shitty is that mint, and how shitty is this coin? . $$ mu_{mint} sim mathrm{Normal}(0,1)$$ $$ sigma_{mint} sim mathrm{Normal}(1, 2)$$ . $$ mu_{coin} sim mathrm{Normal}( mu_{mint}, sigma_{mint})$$ $$ys sim mathrm{Bernoulli}( mu_{coin})$$ .",
            "url": "https://bwgroff.github.io/dspages/julia/turing/2020/12/30/Turing-examples.html",
            "relUrl": "/julia/turing/2020/12/30/Turing-examples.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FastAI with Julia",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/fastai/2020/12/28/fastai-with-julia.html",
            "relUrl": "/julia/fastai/2020/12/28/fastai-with-julia.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Some Julia DS basics",
            "content": "using Gadfly using DataFrames, DataFramesMeta using RDatasets . ┌ Info: Precompiling Gadfly [c91e804a-d5a3-530f-b6f0-dfbca275c004] └ @ Base loading.jl:1278 ┌ Info: Precompiling DataFramesMeta [1313f7d8-7da2-5740-9ea0-a2ca25f37964] └ @ Base loading.jl:1278 . Note that this takes .. a .. really .. long .. time. . iris = dataset(&quot;datasets&quot;, &quot;iris&quot;) . SepalLengthSepalWidthPetalLengthPetalWidthSpecies . Float64Float64Float64Float64Cat… . 150 rows × 5 columns . 15.1 | 3.5 | 1.4 | 0.2 | setosa | . 24.9 | 3.0 | 1.4 | 0.2 | setosa | . 34.7 | 3.2 | 1.3 | 0.2 | setosa | . 44.6 | 3.1 | 1.5 | 0.2 | setosa | . 55.0 | 3.6 | 1.4 | 0.2 | setosa | . 65.4 | 3.9 | 1.7 | 0.4 | setosa | . 74.6 | 3.4 | 1.4 | 0.3 | setosa | . 85.0 | 3.4 | 1.5 | 0.2 | setosa | . 94.4 | 2.9 | 1.4 | 0.2 | setosa | . 104.9 | 3.1 | 1.5 | 0.1 | setosa | . 115.4 | 3.7 | 1.5 | 0.2 | setosa | . 124.8 | 3.4 | 1.6 | 0.2 | setosa | . 134.8 | 3.0 | 1.4 | 0.1 | setosa | . 144.3 | 3.0 | 1.1 | 0.1 | setosa | . 155.8 | 4.0 | 1.2 | 0.2 | setosa | . 165.7 | 4.4 | 1.5 | 0.4 | setosa | . 175.4 | 3.9 | 1.3 | 0.4 | setosa | . 185.1 | 3.5 | 1.4 | 0.3 | setosa | . 195.7 | 3.8 | 1.7 | 0.3 | setosa | . 205.1 | 3.8 | 1.5 | 0.3 | setosa | . 215.4 | 3.4 | 1.7 | 0.2 | setosa | . 225.1 | 3.7 | 1.5 | 0.4 | setosa | . 234.6 | 3.6 | 1.0 | 0.2 | setosa | . 245.1 | 3.3 | 1.7 | 0.5 | setosa | . 254.8 | 3.4 | 1.9 | 0.2 | setosa | . 265.0 | 3.0 | 1.6 | 0.2 | setosa | . 275.0 | 3.4 | 1.6 | 0.4 | setosa | . 285.2 | 3.5 | 1.5 | 0.2 | setosa | . 295.2 | 3.4 | 1.4 | 0.2 | setosa | . 304.7 | 3.2 | 1.6 | 0.2 | setosa | . &vellip;&vellip; | &vellip; | &vellip; | &vellip; | &vellip; | . plot(iris, x=:SepalLength, y=:SepalWidth, color=:Species, Geom.point) . SepalLength -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 9.4 9.6 9.8 10.0 10.2 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 12.0 0 5 10 15 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 setosa versicolor virginica Species h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7.0 -2.5 0.0 2.5 5.0 7.5 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 SepalWidth This also takes a long time - hey I thought Julia was supposed to be fast?! . Ok yeah, so the first time you run a function Julia has to compile it. That depends on the types of the function arguments. .",
            "url": "https://bwgroff.github.io/dspages/julia/basics/2020/12/28/First-Post.html",
            "relUrl": "/julia/basics/2020/12/28/First-Post.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building an Environment",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/reinforcementlearning/2020/12/28/Building-an-Environment.html",
            "relUrl": "/julia/reinforcementlearning/2020/12/28/Building-an-Environment.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Simple Agent",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/reinforcementlearning/2020/12/28/A-Simple-Agent.html",
            "relUrl": "/julia/reinforcementlearning/2020/12/28/A-Simple-Agent.html",
            "date": " • Dec 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bwgroff.github.io/dspages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bwgroff.github.io/dspages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}