{
  
    
        "post0": {
            "title": "A Tour of Models on Binary Data",
            "content": "Introduction . To get myself comfortable with Julia and Turing.jl, I wrote out a bunch of toy models on the simplest of datasets: a simple vector of binary outcomes. To keep myself honest and to have something to refer to, I&#39;m reproducing them here. . The Models . Standard Biased Coin Estimation . Coinflipping is a time honored introductory example for probability exercises, and for good reason. Readily interpretable, amenable to both analytical solutions and manual calculation. Nevermind that &quot;unfair&quot; coins are essentially impossible to make... . We&#39;ll build two models that are essentially the same but have distinct computational profiles. Bernoulli: . $$ begin{align} beta &amp; sim mathrm{Beta}(1,1) y_i &amp; sim mathrm{Bernoulli}( beta) end{align}$$And Binomial: . $$ begin{align} beta &amp; sim mathrm{Beta}(1,1) mathrm{sum}(y) &amp; sim mathrm{Binomial}( mathrm{length}(y), beta) end{align}$$Incorporating Prior Knowledge . The above are interesting in that both depend on the magical constants in the Beta distribution (in this case, all 1s). Personally, I&#39;d need to see a lot of data to accept that a coin had a substantial bias, so in this section we explore the effect of modifying our prior distribution (the parameters of Beta, in this case). . Sequential Dependencies . The preceeding examples are about as far as coin flipping can take us, so we begin exploring dats that has more interesting structure. The first wrinkle we&#39;ll build in is removing the independence between subsequent data elements. This is cheating a little bit, since we&#39;re no longer strictly dealing with binary data as we are also incorporating the order of the elements but this lives in the realm of prior knowledge in my opinion: we believe a priori that the sort order of the data we received is equal to the temporal order of the data as it was generated. . $$ begin{align} beta_0 &amp; sim mathrm{Beta}(1,1) beta_1 &amp; sim mathrm{Beta}(1,1) y_i &amp; sim begin{cases} mathrm{Bernoulli}( beta_0) &amp; y_{i-1} = 0 mathrm{Bernoulli}( beta_1) &amp; y_{i-1} = 1 end{cases} end{align}$$More Complicated Data Generation Processes . using Turing using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(25cm, 8cm) . . Standard Biased Coin Estimation . The simplest possible example: imagine we have a coin that might (or might not... but not really) be fair. Here&#39;s our simple model: . $$ begin{eqnarray} beta &amp; sim&amp; mathrm{Beta}(1,1) y_i &amp; sim&amp; mathrm{Bernoulli}( beta) end{eqnarray}$$ @model function coinflip(y) # prior on p p ~ Beta(1, 1) # updates on p for i in 1:length(y) y[i] ~ Bernoulli(p) end end; . data = [1,1,1,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0] # Settings of the Hamiltonian Monte Carlo (HMC) sampler. iterations = 2000 ϵ = 0.05 τ = 10; . chain_bernoulli = sample(coinflip(data), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_bernoulli), x=:p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 -5 0 5 10 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 Now let&#39;s take a look at an equivalent model that expresses the exact same underlying phenomena. Here&#39;s a model using Binomial in place of Bernoulli: . $$ begin{eqnarray} beta &amp; sim&amp; mathrm{Beta}(1,1) mathrm{heads} &amp; sim&amp; mathrm{Binomial}( mathrm{flips}, beta) end{eqnarray}$$In Turing: . @model function coinflip_binomial(heads::Int64, flips::Int64) # prior on p p ~ Beta(1, 1) # update on p heads ~ Binomial(flips, p) end; . heads = sum(data) flips = length(data) chain_binomial = sample( coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_binomial), x=:p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 -5 0 5 10 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 Note that the inference here should be faster than the Bernoulli case especially as the volume of data increases. The log-likelihood calculations in each sample loop are $O(1)$ versus $O( mathrm{rows})$ in the Bernoulli variant. Let&#39;s take a look with a modestly larger dataset. . long_data = repeat(data, 10); @time sample( coinflip(long_data), HMC(ϵ, τ), iterations, progress=false); . 7.284063 seconds (64.01 M allocations: 1.933 GiB, 6.73% gc time) . heads = sum(long_data) flips = length(long_data) @time sample( coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); . 0.065385 seconds (659.91 k allocations: 46.448 MiB) . Note that the following won&#39;t work, due to design choices in Turing: . @model function coinflip_BROKEN_THIS_WONT_WORK_AAAAAGH(y::Array{Int64}) # prior on p p ~ Beta(1, 1) # updates on p # these lines are the problem. heads = sum(y) # heads is not RV or argument heads ~ Binomial(length(y), p) # so it can&#39;t be on LHS of ~ end; . The reason is that Turing assumes that all variables on the left hand side of ~ are either . random variables (generated by Turing) or | declared as inputs to the function | By way of analogy to Stan, the @model block is only intended to represent the parameter, tranformed parameter and model blocks. As Turing is just Julia code, this is little inconvenience (just do the data transformations in Julia first, it&#39;s more efficient anyway!) but it can be a bit of a surprise and the error message is difficult to interpret. . Going further afield, we can also model $ beta$ on an unconstrained scale ($ mathbb{R}$ instead of the unit interval $[0,1]$) by linking the domains with $ mathrm{logit}^{-1}(x) = frac{e^x}{e^x + 1}$. . $$ begin{eqnarray} rho &amp; sim&amp; mathrm{Beta}(1,1) beta &amp;=&amp; mathrm{logit}^{-1}( rho) y_i &amp; sim&amp; mathrm{Bernoulli}( beta) end{eqnarray}$$On it&#39;s own, this seems like a frivolous transformation but it allows us to incorporate multiple signals into our determination of the probability of a positive result. This is logistic regression. Imagine we have one outcome $y$ that depends on several inputs $x_1, x_2, ldots, x_n$: . $$ begin{eqnarray} rho_i &amp; sim&amp; mathrm{Normal}(0,1) beta &amp;=&amp; mathrm{logit}^{-1}( rho_1 x_1 + rho_2 x_2 + ldots + rho_n x_n) y_i &amp; sim&amp; mathrm{Bernoulli}( beta) end{eqnarray}$$If we didn&#39;t apply this transformation then adding up the components of $ beta$ could lead to values outside of $[0,1]$, ie not in the support of the Bernoulli distribution. . Incorporating Prior Knowledge . Let&#39;s assume you are passingly familiar with the concept of flipping coins. Consequently you reasonably expect nearly every coin to be fair and nearly every person you know to not be Persi Diaconis. . plot distributions http://gadflyjl.org/stable/gallery/statistics/# . With 3 priors (Beta(1,1), Beta(4.9, 5.1), Beta(49, 51)): . Plot comparison of prior distributions | Plot comparison of posterior distributions | Sequential Dependencies . Neato! So, the simplest case is pretty easy to work with. Unfortunately, it also has a bunch of assumptions built into it. The biggest such example is that coin flips are independent trials (or at least assumed to be...). . Assume instead we&#39;re dealing with some arbitrary binary data (a bit stream). In that case, all sorts of surprising behavior is still on the table. For instance, we could have some data that has a lot of streaks (1, 1, 1, 1, 1, 1, ...)? If we retained our incorrect assumption of independence then our point estimate of p will not converge as quickly as expected, and our confidence intervals will be much too narrow. In other words, if we have positive autocorrelation (flipping a 1 is more likely if the preceeding flip was also a 1, and the reverse) then our estimates will just be wrong unless we account for it explicitly. . Let&#39;s generate some data in a way that very explicitly shows the connection to the previous datapoint: . $$ begin{eqnarray} beta_0 &amp; sim&amp; mathrm{Beta}(1,1) beta_1 &amp; sim&amp; mathrm{Beta}(1,1) y_i &amp; sim&amp; begin{cases} mathrm{Bernoulli}( beta_0) &amp; y_{i-1} = 0 mathrm{Bernoulli}( beta_1) &amp; y_{i-1} = 1 end{cases} end{eqnarray}$$ data_autocor = [1] p_1 = 0.85 p_2 = 0.15 for i in 2:1000 if data_autocor[i-1] == 1 p = p_1 else p = p_2 end data_autocor = cat(data_autocor, rand(Bernoulli(p)), dims=1) end . And here&#39;s a model - it&#39;s not an accident that the model code looks so close to the data generating code! . @model function coinflip_autocor(y::Array{Int64}) # prior on p p_1 ~ Beta(1, 1) p_2 ~ Beta(1, 1) y[1] ~ Bernoulli(p_1) # I arbitrarily chose p_1 for i in 2:length(y) if y[i-1] == 1 p = p_1 else p = p_2 end y[i] ~ Bernoulli(p) end end; . chain_autocor = sample(coinflip_autocor(data_autocor), HMC(ϵ, τ), iterations, progress=false)[500:end]; ch_autocor_df = @linq DataFrame(chain_autocor) |&gt; select(:p_1, :p_2) |&gt; DataFrames.stack(); plot(DataFrame(ch_autocor_df), x=:value, color=:variable, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0), Guide.yticks(label = false) ) . value -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 p_1 p_2 variable h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? How to do this with an honest covariance matrix? ie over zip(y[:-1], y[2:])? What about a &quot;rich get richer&quot; scenario? P(1) ~ b + length of streak of 1s . would look good but how to diagnose issues? distributions of streaks! a lot of extremely long streaks, a lot of short streaks... | . | . what about a &quot;machine error&quot; - double readings for both or one of 1s / 0s? . Wide Dependencies: Rich get Richer . Look, we&#39;re long passed the physical analogy making sense for this data so just roll with it for now. The idea for this scenario is a minor extension of the former: every consecutive flip heads makes it more likely the next flip will also be heads, and the reverse for tails. . function streaks(arr) streaks = [] val = arr[1] current_streak = 1 for i in 2:length(arr) next_val = arr[i] if next_val == val current_streak = current_streak + 1 else streaks = cat(streaks, current_streak, dims=1) val = next_val current_streak = 1 end end streaks = cat(streaks, current_streak, dims=1) return streaks end; . streaks([1,1,1,1,2,2,2,2,3,2,1,2,2,2,1]) . 7-element Array{Any,1}: 4 4 1 1 1 3 1 . plot(DataFrame(x=streaks(data_autocor)), x=:x, Geom.histogram) . x -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100 110 120 130 -60 -58 -56 -54 -52 -50 -48 -46 -44 -42 -40 -38 -36 -34 -32 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114 116 118 120 -100 0 100 200 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 -20 0 20 40 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 Many Streams with Discrete Biases . &quot;data on several coins from specific mints. Coin of unknown origin with 10 flips, what is P(heads) on next flip?&quot; . Many Streams with Continuous Biases . All coins come from one rather shitty mint. How shitty is that mint, and how shitty is this coin? . $$ begin{eqnarray} mu_{mint} &amp; sim&amp; mathrm{Normal}(0,1) sigma_{mint} &amp; sim&amp; mathrm{Normal}(1, 2) mu_{coin} &amp; sim&amp; mathrm{Normal}( mu_{mint}, sigma_{mint}) ys &amp; sim&amp; mathrm{Bernoulli}( mu_{coin}) end{eqnarray}$$Compare naive estimate and regularized estimate. Note: regularization lets us &quot;share error&quot; between . &quot;Seasonal&quot; data . e.g. hourly aggregate from a motion sensor in a shop: &quot;there was motion in the shop over the past hour&quot; . Extremely sparse data? .",
            "url": "https://bwgroff.github.io/dspages/julia/turing/2020/12/30/Turing-examples.html",
            "relUrl": "/julia/turing/2020/12/30/Turing-examples.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FastAI with Julia",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/fastai/2020/12/28/fastai-with-julia.html",
            "relUrl": "/julia/fastai/2020/12/28/fastai-with-julia.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Some Julia DS basics",
            "content": "using Gadfly using DataFrames, DataFramesMeta using RDatasets . ┌ Info: Precompiling Gadfly [c91e804a-d5a3-530f-b6f0-dfbca275c004] └ @ Base loading.jl:1278 ┌ Info: Precompiling DataFramesMeta [1313f7d8-7da2-5740-9ea0-a2ca25f37964] └ @ Base loading.jl:1278 . Note that this takes .. a .. really .. long .. time. . iris = dataset(&quot;datasets&quot;, &quot;iris&quot;) . SepalLengthSepalWidthPetalLengthPetalWidthSpecies . Float64Float64Float64Float64Cat… . 150 rows × 5 columns . 15.1 | 3.5 | 1.4 | 0.2 | setosa | . 24.9 | 3.0 | 1.4 | 0.2 | setosa | . 34.7 | 3.2 | 1.3 | 0.2 | setosa | . 44.6 | 3.1 | 1.5 | 0.2 | setosa | . 55.0 | 3.6 | 1.4 | 0.2 | setosa | . 65.4 | 3.9 | 1.7 | 0.4 | setosa | . 74.6 | 3.4 | 1.4 | 0.3 | setosa | . 85.0 | 3.4 | 1.5 | 0.2 | setosa | . 94.4 | 2.9 | 1.4 | 0.2 | setosa | . 104.9 | 3.1 | 1.5 | 0.1 | setosa | . 115.4 | 3.7 | 1.5 | 0.2 | setosa | . 124.8 | 3.4 | 1.6 | 0.2 | setosa | . 134.8 | 3.0 | 1.4 | 0.1 | setosa | . 144.3 | 3.0 | 1.1 | 0.1 | setosa | . 155.8 | 4.0 | 1.2 | 0.2 | setosa | . 165.7 | 4.4 | 1.5 | 0.4 | setosa | . 175.4 | 3.9 | 1.3 | 0.4 | setosa | . 185.1 | 3.5 | 1.4 | 0.3 | setosa | . 195.7 | 3.8 | 1.7 | 0.3 | setosa | . 205.1 | 3.8 | 1.5 | 0.3 | setosa | . 215.4 | 3.4 | 1.7 | 0.2 | setosa | . 225.1 | 3.7 | 1.5 | 0.4 | setosa | . 234.6 | 3.6 | 1.0 | 0.2 | setosa | . 245.1 | 3.3 | 1.7 | 0.5 | setosa | . 254.8 | 3.4 | 1.9 | 0.2 | setosa | . 265.0 | 3.0 | 1.6 | 0.2 | setosa | . 275.0 | 3.4 | 1.6 | 0.4 | setosa | . 285.2 | 3.5 | 1.5 | 0.2 | setosa | . 295.2 | 3.4 | 1.4 | 0.2 | setosa | . 304.7 | 3.2 | 1.6 | 0.2 | setosa | . &vellip;&vellip; | &vellip; | &vellip; | &vellip; | &vellip; | . plot(iris, x=:SepalLength, y=:SepalWidth, color=:Species, Geom.point) . SepalLength -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 9.4 9.6 9.8 10.0 10.2 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 12.0 0 5 10 15 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 setosa versicolor virginica Species h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7.0 -2.5 0.0 2.5 5.0 7.5 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 SepalWidth This also takes a long time - hey I thought Julia was supposed to be fast?! . Ok yeah, so the first time you run a function Julia has to compile it. That depends on the types of the function arguments. .",
            "url": "https://bwgroff.github.io/dspages/julia/basics/2020/12/28/First-Post.html",
            "relUrl": "/julia/basics/2020/12/28/First-Post.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building an Environment",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/reinforcementlearning/2020/12/28/Building-an-Environment.html",
            "relUrl": "/julia/reinforcementlearning/2020/12/28/Building-an-Environment.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Simple Agent",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/reinforcementlearning/2020/12/28/A-Simple-Agent.html",
            "relUrl": "/julia/reinforcementlearning/2020/12/28/A-Simple-Agent.html",
            "date": " • Dec 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bwgroff.github.io/dspages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bwgroff.github.io/dspages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}