{
  
    
        "post0": {
            "title": "Binary Data Models",
            "content": "Coinflipping is a time honored example for probability exercises, and for good reason. It is readily interpretable while providing ample opportunity for more complex scenarios. Nevermind that &quot;unfair&quot; coins are essentially impossible to make... In any case, a few examples of coin flipping! . using Turing using Gadfly using DataFrames, DataFramesMeta . . Scenario: Standard Fair(?) Coin Estimation . The simplest possible example: imagine we have a coin that might (or might not!) be fair. blah blah . @model function coinflip(y) # prior on p p ~ Beta(1, 1) # updates on p for i in 1:length(y) y[i] ~ Bernoulli(p) end end; . data = [1,1,1,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0] # Settings of the Hamiltonian Monte Carlo (HMC) sampler. iterations = 2000 ϵ = 0.05 τ = 10; . chain_bernoulli = sample(coinflip(data), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_bernoulli), x=:p, Geom.histogram) . p -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 -1 0 1 2 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100 110 -50 -48 -46 -44 -42 -40 -38 -36 -34 -32 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 -50 0 50 100 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Just to be clear, there are equivalent models that express the exact same underlying phenomena. Here&#39;s one example using Binomial in place of Bernoulli: . @model function coinflip_binomial(heads::Int64, flips::Int64) # prior on p p ~ Beta(2, 2) # update on p heads ~ Binomial(flips, p) end; . heads = sum(data) flips = length(data) chain_binomial = sample(coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_binomial), x=:p, Geom.histogram) . p -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 -1 0 1 2 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 -40 -38 -36 -34 -32 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 -50 0 50 100 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Note that the inference here should be faster than the Bernoulli case especially as the volume of data increases. The log-likelihood calculations in each sample loop are $O(1)$ versus $O( mathrm{rows})$ in the Bernoulli variant. Let&#39;s take a look with a modestly larger dataset. . long_data = repeat(data, 100); @time sample(coinflip(long_data), HMC(ϵ, τ), iterations, progress=false); . 80.996511 seconds (634.32 M allocations: 18.929 GiB, 7.05% gc time) . heads = sum(long_data) flips = length(long_data) @time sample(coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations); . Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00 . 0.524443 seconds (1.26 M allocations: 74.442 MiB, 4.14% gc time) . Note that the following won&#39;t work, due to design choices in Turing: . @model function coinflip_BROKEN_THIS_WONT_WORK_AAAAAGH(y::Array{Int64}) # prior on p p ~ Beta(1, 1) # updates on p heads = sum(y) # these lines are heads ~ Binomial(length(y), p) # the problem end; . The reason is that Turing assumes that all variables on the left hand side of ~ are either . random variables (generated by Turing) or | declared as inputs to the function | By way of analogy to Stan, the @model block is only intended to represent the parameter, tranformed parameter and model blocks. As Turing is just Julia code, this is little inconvenience (just do the data transformations in Julia first!) but it can be a bit of a surprise and the error message is difficult to interpret. . Scenario: Autocorrelation . Neato! So, the simplest case is pretty easy to work with. Unfortunately, it also has a bunch of assumptions built into it. The biggest such example is that coin flips are independent trials (or at least assumed to be...). . Assume instead we&#39;re dealing with some arbitrary binary data (a bit stream). In that case, all sorts of surprising behavior is still on the table. For instance, we could have some data that has a lot of streaks (1, 1, 1, 1, 1, 1, ...)? If we retained our incorrect assumption of independence then our point estimate of p will not converge as quickly as expected, and our confidence intervals will be much too narrow. In other words, if we have positive autocorrelation (flipping a 1 is more likely if the preceeding flip was also a 1, and the reverse) then our estimates will just be wrong unless we account for it explicitly. . Let&#39;s generate some data in a way that very explicitly shows the connection to the previous datapoint: . data_autocor = [1] p_1 = 0.85 p_2 = 0.15 for i in 2:1000 if data_autocor[i-1] == 1 p = p_1 else p = p_2 end data_autocor = cat(data_autocor, rand(Bernoulli(p)), dims=1) end . And here&#39;s a model - it&#39;s not an accident that the model code looks so close to the data generating code! . @model function coinflip_autocor(y::Array{Int64}) # prior on p p_1 ~ Beta(1, 1) p_2 ~ Beta(1, 1) y[1] ~ Bernoulli(p_1) for i in 2:length(y) if y[i-1] == 1 p = p_1 else p = p_2 end y[i] ~ Bernoulli(p) end end; . chain_autocor = sample(coinflip_autocor(data_autocor), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_autocor), layer(x=:p_1, Geom.histogram), layer(x=:p_2, Geom.histogram)) . p_1 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -250 -200 -150 -100 -50 0 50 100 150 200 250 300 350 400 450 -200 -190 -180 -170 -160 -150 -140 -130 -120 -110 -100 -90 -80 -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 -200 0 200 400 -200 -180 -160 -140 -120 -100 -80 -60 -40 -20 0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 How to do this with an honest covariance matrix? ie over zip(y[:-1], y[2:])? What about a &quot;rich get richer&quot; scenario? P(1) ~ b + length of streak of 1s . would look good but how to diagnose issues? distributions of streaks! a lot of extremely long streaks, a lot of short streaks... | . | . what about a &quot;machine error&quot; - double readings for both or one of 1s / 0s? . Scenario: Rich get Richer . Look, we&#39;re long passed the physical analogy making sense for this data so just roll with it for now. The idea for this scenario is a minor extension of the former: every consecutive flip heads makes it more likely the next flip will also be heads, and the reverse for tails. . function streaks(arr) streaks = [] val = arr[1] current_streak = 1 for i in 2:length(arr) next_val = arr[i] if next_val == val current_streak = current_streak + 1 else streaks = cat(streaks, current_streak, dims=1) val = next_val current_streak = 1 end end streaks = cat(streaks, current_streak, dims=1) return streaks end; . streaks([1,1,1,1,2,2,2,2,3,2,1,2,2,2,1]) . 7-element Array{Any,1}: 4 4 1 1 1 3 1 . plot(DataFrame(x=streaks(data_autocor)), x=:x, Geom.histogram) . x -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 -40 -38 -36 -34 -32 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 -50 0 50 100 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 -20 0 20 40 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 Scenario: Many Coins of Discrete Biases . &quot;data on several coins from specific mints. Coin of unknown origin with 10 flips, what is P(heads) on next flip?&quot; . Scenario: Many Coins with Continuous Biases . All coins come from one rather shitty mint. How shitty is that mint, and how shitty is this coin? . $$ mu_{mint} sim mathrm{Normal}(0,1)$$ $$ sigma_{mint} sim mathrm{Normal}(1, 2)$$ . $$ mu_{coin} sim mathrm{Normal}( mu_{mint}, sigma_{mint})$$ $$ys sim mathrm{Bernoulli}( mu_{coin})$$ . Compare naive estimate and regularized estimate. Note: regularization lets us &quot;share error&quot; between . &quot;Seasonal&quot; data . e.g. hourly aggregate from a motion sensor in a shop: &quot;there was motion in the shop over the past hour&quot; . Extremely sparse data? .",
            "url": "https://bwgroff.github.io/dspages/julia/turing/2020/12/30/Turing-examples.html",
            "relUrl": "/julia/turing/2020/12/30/Turing-examples.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FastAI with Julia",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/fastai/2020/12/28/fastai-with-julia.html",
            "relUrl": "/julia/fastai/2020/12/28/fastai-with-julia.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Some Julia DS basics",
            "content": "using Gadfly using DataFrames, DataFramesMeta using RDatasets . ┌ Info: Precompiling Gadfly [c91e804a-d5a3-530f-b6f0-dfbca275c004] └ @ Base loading.jl:1278 ┌ Info: Precompiling DataFramesMeta [1313f7d8-7da2-5740-9ea0-a2ca25f37964] └ @ Base loading.jl:1278 . Note that this takes .. a .. really .. long .. time. . iris = dataset(&quot;datasets&quot;, &quot;iris&quot;) . SepalLengthSepalWidthPetalLengthPetalWidthSpecies . Float64Float64Float64Float64Cat… . 150 rows × 5 columns . 15.1 | 3.5 | 1.4 | 0.2 | setosa | . 24.9 | 3.0 | 1.4 | 0.2 | setosa | . 34.7 | 3.2 | 1.3 | 0.2 | setosa | . 44.6 | 3.1 | 1.5 | 0.2 | setosa | . 55.0 | 3.6 | 1.4 | 0.2 | setosa | . 65.4 | 3.9 | 1.7 | 0.4 | setosa | . 74.6 | 3.4 | 1.4 | 0.3 | setosa | . 85.0 | 3.4 | 1.5 | 0.2 | setosa | . 94.4 | 2.9 | 1.4 | 0.2 | setosa | . 104.9 | 3.1 | 1.5 | 0.1 | setosa | . 115.4 | 3.7 | 1.5 | 0.2 | setosa | . 124.8 | 3.4 | 1.6 | 0.2 | setosa | . 134.8 | 3.0 | 1.4 | 0.1 | setosa | . 144.3 | 3.0 | 1.1 | 0.1 | setosa | . 155.8 | 4.0 | 1.2 | 0.2 | setosa | . 165.7 | 4.4 | 1.5 | 0.4 | setosa | . 175.4 | 3.9 | 1.3 | 0.4 | setosa | . 185.1 | 3.5 | 1.4 | 0.3 | setosa | . 195.7 | 3.8 | 1.7 | 0.3 | setosa | . 205.1 | 3.8 | 1.5 | 0.3 | setosa | . 215.4 | 3.4 | 1.7 | 0.2 | setosa | . 225.1 | 3.7 | 1.5 | 0.4 | setosa | . 234.6 | 3.6 | 1.0 | 0.2 | setosa | . 245.1 | 3.3 | 1.7 | 0.5 | setosa | . 254.8 | 3.4 | 1.9 | 0.2 | setosa | . 265.0 | 3.0 | 1.6 | 0.2 | setosa | . 275.0 | 3.4 | 1.6 | 0.4 | setosa | . 285.2 | 3.5 | 1.5 | 0.2 | setosa | . 295.2 | 3.4 | 1.4 | 0.2 | setosa | . 304.7 | 3.2 | 1.6 | 0.2 | setosa | . &vellip;&vellip; | &vellip; | &vellip; | &vellip; | &vellip; | . plot(iris, x=:SepalLength, y=:SepalWidth, color=:Species, Geom.point) . SepalLength -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 9.4 9.6 9.8 10.0 10.2 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 12.0 0 5 10 15 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 setosa versicolor virginica Species h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7.0 -2.5 0.0 2.5 5.0 7.5 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 SepalWidth This also takes a long time - hey I thought Julia was supposed to be fast?! . Ok yeah, so the first time you run a function Julia has to compile it. That depends on the types of the function arguments. .",
            "url": "https://bwgroff.github.io/dspages/julia/basics/2020/12/28/First-Post.html",
            "relUrl": "/julia/basics/2020/12/28/First-Post.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building an Environment",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/reinforcementlearning/2020/12/28/Building-an-Environment.html",
            "relUrl": "/julia/reinforcementlearning/2020/12/28/Building-an-Environment.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Simple Agent",
            "content": "",
            "url": "https://bwgroff.github.io/dspages/julia/reinforcementlearning/2020/12/28/A-Simple-Agent.html",
            "relUrl": "/julia/reinforcementlearning/2020/12/28/A-Simple-Agent.html",
            "date": " • Dec 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bwgroff.github.io/dspages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bwgroff.github.io/dspages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}