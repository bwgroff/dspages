{
  
    
        "post0": {
            "title": "Some More Models on Binary Data",
            "content": "To get myself comfortable with Julia and Turing.jl, I wrote out a bunch of toy models on the simplest of datasets: a simple vector of binary outcomes. To keep myself honest and to have something to refer to, I&#39;m reproducing them here. . using Turing using Bijectors using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(900px, 300px) . . Sequential Dependencies . The preceeding examples are about as far as coin flipping can take us, so let&#39;s begin exploring data that has more interesting structure. The first challenge we&#39;ll tackle is dependence between subsequent data elements. This is cheating a little bit, since we&#39;re no longer strictly dealing with binary data as we are also incorporating the order of the elements. In my opinion, this lives in the realm of prior knowledge: we believe a priori that the sort order of the data we received is equal to the temporal order of the data as it was generated. . Here is a very simple model to capture this dependency: . $$ begin{aligned} beta_0 &amp; sim mathrm{Beta}(1,1) beta_1 &amp; sim mathrm{Beta}(1,1) y_i &amp; sim begin{cases} mathrm{Bernoulli}( beta_0) &amp; y_{i-1} = 0 mathrm{Bernoulli}( beta_1) &amp; y_{i-1} = 1 end{cases} end{aligned}$$Note that if we incorrectly accepted an assumption of independence as in our previous models then our point estimate of p will not converge as quickly as expected, and our confidence intervals will be much too narrow. In other words, if we have positive autocorrelation (flipping a 1 is more likely if the preceeding flip was also a 1, and the reverse) then our estimates will just be wrong unless we account for it explicitly. . Let&#39;s generate some data in a way that very explicitly shows the connection to the previous datapoint: . data_autocor = [1] p_1 = 0.85 p_2 = 0.35 for i in 2:1000 if data_autocor[i-1] == 1 p = p_1 else p = p_2 end data_autocor = cat(data_autocor, rand(Bernoulli(p)), dims=1) end . And here&#39;s a model - it&#39;s not an accident that the model code looks so close to the data generating code! . @model function coinflip_autocor(y::Array{Int64}) # prior on p p_1 ~ Beta(1, 1) p_2 ~ Beta(1, 1) y[1] ~ Bernoulli(p_1) # I arbitrarily chose p_1 for i in 2:length(y) if y[i-1] == 1 p = p_1 else p = p_2 end y[i] ~ Bernoulli(p) end end; . Here are the estimates of the two parameters: . chain_autocor = sample(coinflip_autocor(data_autocor), HMC(ϵ, τ), iterations, progress=false)[500:end]; ch_autocor_df = @linq DataFrame(chain_autocor) |&gt; select(:p_1, :p_2) |&gt; DataFrames.stack(); plot(DataFrame(ch_autocor_df), x=:value, color=:variable, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0), Guide.yticks(label = false) ) . . value -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 p_1 p_2 variable h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? We have less certainty about $p_2$ than $p_1$. This is predictable from the data generation process: because heads are more likely to beget heads than tails are to beget tails (ie $p_1$ is closer to 1 than $p_2$ is to 0), we have more examples drawing from the $p_1$ distribution. . How to do this with an honest covariance matrix? ie over zip(y[:-1], y[2:])? What about a &quot;rich get richer&quot; scenario? P(1) ~ b + length of streak of 1s . would look good but how to diagnose issues? distributions of streaks! a lot of extremely long streaks, a lot of short streaks... | . | . what about a &quot;machine error&quot; - double readings for both or one of 1s / 0s? . Wide Dependencies: Rich get Richer . Look, we&#39;re long passed the physical analogy making sense for this data so just roll with it for now. The idea for this scenario is a minor extension of the former: every consecutive flip heads makes it more likely the next flip will also be heads, and the reverse for tails. . function streaks(arr) streaks = [] val = arr[1] current_streak = 1 for i in 2:length(arr) next_val = arr[i] if next_val == val current_streak = current_streak + 1 else streaks = cat(streaks, current_streak, dims=1) val = next_val current_streak = 1 end end streaks = cat(streaks, current_streak, dims=1) return streaks end; . plot(DataFrame(x=streaks(data_autocor)), x=:x, Geom.histogram) . x -40 -30 -20 -10 0 10 20 30 40 50 60 70 -30 -29 -28 -27 -26 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 -30 0 30 60 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100 110 120 130 -60 -58 -56 -54 -52 -50 -48 -46 -44 -42 -40 -38 -36 -34 -32 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114 116 118 120 -100 0 100 200 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 Many Streams with Discrete Biases . &quot;data on several coins from specific mints. Coin of unknown origin with 10 flips, what is P(heads) on next flip?&quot; . Many Streams with Continuous Biases . All coins come from one rather shitty mint. How shitty is that mint, and how shitty is this coin? . $$ begin{aligned} mu_{mint} &amp; sim mathrm{Normal}(0,1) sigma_{mint} &amp; sim mathrm{Normal}(1, 2) mu_{coin} &amp; sim mathrm{Normal}( mu_{mint}, sigma_{mint}) ys &amp; sim mathrm{Bernoulli}( mu_{coin}) end{aligned}$$Compare naive estimate and regularized estimate. Note: regularization lets us &quot;share error&quot; between . &quot;Seasonal&quot; data . e.g. hourly aggregate from a motion sensor in a shop: &quot;there was motion in the shop over the past hour&quot; . Extremely sparse data? .",
            "url": "https://bwgroff.github.io/dspages/julia/turing/2021/01/03/Turing-Examples-2.html",
            "relUrl": "/julia/turing/2021/01/03/Turing-Examples-2.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Some Simple Models on Binary Data",
            "content": "To get myself comfortable with Julia and Turing.jl, I wrote out a bunch of toy models on the simplest of datasets: a vector of binary outcomes. To keep myself honest and to have something to refer to, I&#39;m reproducing them here. . using Turing using Bijectors using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(900px, 300px) . . Standard Biased Coin Estimation . Coinflipping is a time-honored introductory example for probability, and for good reason. Readily interpretable, amenable to both analytical solutions and manual calculation. Nevermind that &quot;unfair&quot; coins are essentially impossible to make... . We&#39;ll build two models that are essentially the same but have distinct computational profiles. Bernoulli: . $$ begin{aligned} beta &amp; sim mathrm{Beta}(1,1) y_i &amp; sim mathrm{Bernoulli}( beta) end{aligned}$$And Binomial: . $$ begin{aligned} beta &amp; sim mathrm{Beta}(1,1) mathrm{sum}(y) &amp; sim mathrm{Binomial}( mathrm{length}(y), beta) end{aligned}$$ Bernoulli Model . First thing to do is to translate the Bernoulli code above into a @model in Turing: . @model function coinflip(y) # prior on p p ~ Beta(1, 1) # updates on p for i in 1:length(y) y[i] ~ Bernoulli(p) end end; . It looks more or less like the description above, with the loop for i in 1:length(y) serving the same purpose as the subscript of $y_i$. . data = [1,1,1,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0] # Settings of the Hamiltonian Monte Carlo (HMC) sampler. iterations = 2000 ϵ = 0.05 τ = 10; chain_bernoulli = sample(coinflip(data), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_bernoulli), x=:p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 -5 0 5 10 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 Binomial Model . Now let&#39;s take a look at the equivalent model using a Binomial distribution that expresses the exact same underlying phenomena. In Turing: . @model function coinflip_binomial(heads::Int64, flips::Int64) # prior on p p ~ Beta(1, 1) # update on p heads ~ Binomial(flips, p) end; . heads = sum(data) flips = length(data) chain_binomial = sample( coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_binomial), x=:p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 -5 0 5 10 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 Other Parameter Spaces . Going further afield, we can also model $ beta$ on an unconstrained scale ($ mathbb{R}$ instead of the unit interval $[0,1]$) by linking the domains with $ mathrm{logit}^{-1}(x) = frac{e^x}{e^x + 1}$. . $$ begin{aligned} rho &amp; sim mathrm{Beta}(1,1) beta &amp;= mathrm{logit}^{-1}( rho) y_i &amp; sim mathrm{Bernoulli}( beta) end{aligned}$$On it&#39;s own, this seems like a frivolous transformation but it allows us to incorporate multiple signals into our determination of the probability of a positive result. Turns out, this is just logistic regression. Imagine we have one outcome $y$ that depends on several inputs $x_1, x_2, ldots, x_n$: . $$ begin{aligned} rho_i &amp; sim mathrm{Normal}(0,1) beta &amp;= mathrm{logit}^{-1}( rho_1 x_1 + rho_2 x_2 + ldots + rho_n x_n) y_i &amp; sim mathrm{Bernoulli}( beta) end{aligned}$$Because we pass the regression product through $ mathrm{logit}^{-1}$, we know the value of $ beta$ will be between 0 and 1. If we didn&#39;t apply this transformation then adding up the components of $ beta$ could lead to values outside of that range, (and therefore not in the support of the Bernoulli distribution) which would cause the computation to fail. . The same approach works for the Binomial model: . $$ begin{aligned} rho_i &amp; sim mathrm{Normal}(0,1) beta &amp;= mathrm{logit}^{-1}( rho_1 x_1 + rho_2 x_2 + ldots + rho_n x_n) mathrm{sum}(y) &amp; sim mathrm{Binomial}( mathrm{length}(y), beta) end{aligned}$$Turing allows us to create a logit function in two ways. One is in pure Julia code (note I didn&#39;t apply a type assertion to the x argument: things can get messy here with automatic differentiation): . function invlogit(x) ex = exp(x) return ex / (1 + ex) end; . It&#39;s also possible to use some transformations that Turing uses internally from the Bijectors.jl library. Some sampling algorithms like HMC require an unbounded sampling space to (or something like it subject to Float64 constraints...). In order to enable that, the Bijectors library creates functions that map (smoothly) between continuous spaces. In . logit = bijector(Beta()) # bijection: (0, 1) → ℝ inv_logit = inv(logit) # bijection: ℝ → (0, 1) @model function coinflip_invlogit(heads::Int64, flips::Int64) # prior on p logit_p ~ Normal(0, 1) # or any unbounded distribution # update on p heads ~ Binomial(flips, invlogit(logit_p)) end; . heads = sum(data) flips = length(data) chain_invlogit = sample( coinflip_invlogit(heads, flips), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_invlogit), x=:logit_p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(ymin=0.0) ) . . logit_p -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 -6.0 -5.8 -5.6 -5.4 -5.2 -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 -6 -3 0 3 6 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 -2 0 2 4 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 Note the new x-axis which is the transformation of the previous axis. Here are some helpful values: . $$ begin{aligned} mathrm{logit}^{-1}(1.0) &amp;= 0.73 mathrm{logit}^{-1}(0) &amp;= frac{1}{2} mathrm{logit}^{-1}(-1.0) &amp;= 0.27 end{aligned}$$ Performance Difference . Despite essentially describing the same model, performance will differ significantly beteen the Binomial and the Bernoulli cases especially as the volume of data increases. The log-likelihood calculations in each sample loop are $O(1)$ versus $O( mathrm{rows})$, respectively. Here&#39;s a comparison with a modestly larger dataset: . long_data = repeat(data, 10); @time sample( coinflip(long_data), HMC(ϵ, τ), iterations, progress=false); . 7.658380 seconds (64.01 M allocations: 1.933 GiB, 6.88% gc time) . heads = sum(long_data) flips = length(long_data) @time sample( coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); . 0.105582 seconds (659.91 k allocations: 46.448 MiB, 24.33% gc time) . Caution! . Just a little &quot;gotcha&quot; I ran into. The following model won&#39;t work, due to design choices in Turing: . @model function coinflip_BROKEN_THIS_WONT_WORK_AAAAAGH(y::Array{Int64}) # prior on p p ~ Beta(1, 1) # updates on p # these lines are the problem. heads = sum(y) # heads is not RV or argument heads ~ Binomial(length(y), p) # so it can&#39;t be on LHS of ~ end; . The reason is that Turing assumes that all variables on the left hand side of ~ are either . random variables (generated by Turing) or | declared as inputs to the function | By way of analogy to Stan, the @model block of Turing is only intended to represent the parameter, tranformed parameter and model blocks of Stan. As Turing is just Julia code, this is little inconvenience (just do the data transformations in Julia first, it&#39;s more efficient anyway!) but it can be a bit of a surprise and the error message is difficult to interpret. . Incorporating Prior Knowledge . The above models depend on unexplained constants in the Beta distribution ($ mathrm{Beta}(1,1)$ vs, say, $ mathrm{Beta}(13.859, pi^e)$). Those values are left unjustified and here we&#39;ll put some more thought into them. . Let&#39;s assume you are passingly familiar with the concept of flipping coins. Consequently you reasonably expect nearly every coin to be fair and nearly every person you know to not be Persi Diaconis. Personally, I&#39;d need to see a lot of data to accept that a coin had a substantial bias, so in this section we explore to bake that prior knowledge into our model as our prior distribution (the parameters of Beta, in this case). This is a small change to the model but we&#39;ll explore the impacts of a few alternatives for constraints on $ beta$: . $$ begin{aligned} beta &amp; sim mathrm{Beta}(1,1) &amp; mathrm{ versus} beta &amp; sim mathrm{Beta}(5,5) &amp; mathrm{ versus} beta &amp; sim mathrm{Beta}(50,50) &amp; mathrm{ versus} beta &amp; sim mathrm{Beta}(15,5)&amp; mathrm{ (for fun)} end{aligned}$$ plot distributions http://gadflyjl.org/stable/gallery/statistics/# . With 3 priors (Beta(1,1), Beta(4.9, 5.1), Beta(49, 51)): . Plot comparison of prior distributions | Plot comparison of posterior distributions |",
            "url": "https://bwgroff.github.io/dspages/julia/turing/2020/12/30/Turing-Examples-1.html",
            "relUrl": "/julia/turing/2020/12/30/Turing-Examples-1.html",
            "date": " • Dec 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bwgroff.github.io/dspages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bwgroff.github.io/dspages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}