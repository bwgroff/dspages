{
  
    
        "post0": {
            "title": "Item Response Models",
            "content": "using Turing using Bijectors using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(900px, 300px) . . Item response models are used to simultaneously make inferences about two interacting populations. Commonly, a population of test questions and a population of test takers (students) with the result (success/failure) of each student on each question they&#39;ve seen. This is an interesting problem in that even in the basic case: . students have different levels of aptitude | questions have different levels of difficulty | not every student sees every question | not every question needs to be seen by the same number of students | we should be able to make relative inferences between students/questions that have no overlapping questions/students | the data is nonetheless fairly simple: [correct (Boolean), student_id (categorical), question_id (categorical] | . Item Response may seem very similar to collaborative filtering, so it&#39;s worth disambiguating. . Collaborative filtering aims to complete a sparsely determined preferences/ratings matrix of consumer - item scores (e.g. &quot;3.5 :star:&quot;) $M$. A common approach is alternating least squares which iteratively factors the matrix into a product-feature matrix $P$ and a customer-preference matrix $C$. The goal is to create these so that their product &quot;accurately completes&quot; $M$, ie if $CP = overline{M}$ then the difference $M - overline{M}$ is small wherever we have entries for $M$ (remember, $M$ is incomplete). . A key fact is that the matrix $ overline{M}$ (the list of recommendations) is the important output here, and the factors $C$ and $P$ are intriguing but not critical. This is different from Item Response where the background variables describing difficulty and aptitude for each question, student are the primary desired outputs (but we could infer $P( mathrm{correct} | mathrm{student_id}, mathrm{question_id})$ for unseen pairings!). . The other distinction worth mentioning is that the IR models have enormous flexibility in how they inform the probability of success, as we&#39;ll see. Collaborative filtering, at least with ALS, is just optimizing a matrix factorization task. Since $ overline{M} = CP$, the user-product score can only be the dot product of the product-feature vector and the customer-preference vector, it attempts to encode &quot;how well do the consumer preferences overlap with the product features.&quot; . Stan users guide - IRT .",
            "url": "http://www.bwg.is/julia/turing/itemresponse/2021/01/05/Item-Response-I.html",
            "relUrl": "/julia/turing/itemresponse/2021/01/05/Item-Response-I.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Customer LTV - Buy til You Die",
            "content": "using Turing using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(900px, 300px) . . Customer Lifetime Value (LTV or CLTV) is the total dollar value a consumer will spend at a business throughout their life. The concept is as important as the definition is straightforward - businesses very often want to know which consumers are their whales and which are eating up their marketing or infrastructure budgets with little or no value returned. This is pretty tricky and there are a few approaches you can take: . Observational . Naive calculation. The following will give you an average that is delightfully simple but tragically wrong: . $$ mathrm{LTV} = frac{1}{| mathrm{Customers}|} sum_{ mathrm{orders}} mathrm{Order Value}$$ . Assuming (hmm) that LTV is constant over time, this will converge to the true average LTV value as customers churn (and thus achieve their final lifetime value). New customers will continue to weigh the average down and make it an underestimate. There are some of these sort of equations floating around the tubes. . Wait and see. Simialr algorithm to the above, the major difference is applying this to only a small cohort from a brief window in time. Just follow along with that group and add up how much they spend. This is simple and will get to the true LTV of that cohort faster but it&#39;s still typically too slow to be useful. By the time you know, it&#39;s months/quarters/years later (depending on the churn characteristics of your product) and most insights you might glean are no longer relevant to your product roadmap. . Modeled . Machine Learning :tada:. There are a bunch of ML approaches here that can be found relatively easily online (but apparently not easy enough for me to find them again to include here). IIRC, one was using a random forest (or GBM, or whatever) to predict . $$P( mathrm{purchase in next period}| mathcal{D})$$ . and then in a second stage model (conditioned on the purchase outcome) predict the order value of said purchase. . It&#39;s a reasonably standard approach: decompose the problem into churn, expected future purchases, and expected value per purchase. There are a bunch of approaches that are tailored to this decomposition by breaking down the inputs into the so called RFM metrics: . Recency: time since the last purchase, | Frequency: number of purchases per time period, | Monetary value: average order value. | . Note that we&#39;ll use days for the time scale. . Buy &#39;til You Die. https://www.zdnet.com/article/nikes-purchase-of-analytics-firm-zodiac-highlights-focus-on-customer-lifetime-value/ http://www.brucehardie.com/papers/bgnbd_2004-04-20.pdf . Custom Model. That&#39;s what we&#39;re going to do! Fader and Hardie do a great job of making their work look harder than necessary so I can&#39;t be bothered to decode it (and anyway, Alex did a great job). That said, I&#39;m going to take what I believe to be a similar approach: . Estimate churn based on Recency and Frequency. | Set up a super simple survival model to understand the expected number of future purchases using sample from (1) as the churn signal. | Scale by Monetary value. | By building these out independently we can understood the whole model but first figuring it out component-by-component. It also provides a quick way to make single-component adjustments that might be important. For instance, some retailers have an extremely wide spread of possible order values (e.g. Walmart, you can buy a stick of gum or probably a boat or something). If there are orders-of-magnitude differences in purchase value then you better model that out so you know exactly which consumers are likely to find themselves that lucrative long tail. In my experience, lognormal is a decent start but the tail is still too light. . Our Models . Churn . We sample when we expect the customer&#39;s next purchase to occur based on what we&#39;ve observed of their frequency, then we compare that to how long it&#39;s been since they purchased. If we expected them to purchase already, we count them as churned. Note that we don&#39;t have any kind of regularization and just assume F is a fine number for us. Exercise for the reader to make that more stable :smile:. . $$ begin{aligned} mathrm{next purchase} &amp; sim mathrm{Exponential}(F) mathrm{active} &amp;= R &lt; mathrm{next purchase} end{aligned} $$Survival . We&#39;d like to then take the inferences above and use them to understand churn as a function of time, or perhaps number of orders. In other words: . $$P( mathrm{churned}_{t=i} | mathrm{active}_{t=i-1})$$ . Here we find some wrinkles. Most notably, what to do with consumers that have recently purchased and we don&#39;t know if they are going to churn before the next purchase? This is called censoring, which comes in many directional varieties and this variety is called right-censoring (on the &quot;right&quot; side of our time interval, we don&#39;t yet have data on the outcome). . $$ begin{aligned} rho &amp; sim mathrm{Beta}(1,1) mathrm{purchases}_{uncensored} &amp; sim mathrm{Geometric( rho)} mathrm{purchases}_{censored} &amp; sim mathrm{Geometric( rho ?)} ( mathrm{Future purchases}) &amp; sim begin{cases} mathrm{Geometric}( rho) &amp; mathrm{if active} mathrm{Dirac}(0) &amp; mathrm{otherwise} end{cases} end{aligned} $$LTV . $$ begin{aligned} mathrm{Future value} &amp;= mathrm{Future purchases} * mathrm{AOV} mathrm{Lifetime value} &amp;= mathrm{Future value} + mathrm{Past value} end{aligned} $$ struct CustomerData recency::Int64 frequency::Float64 money::Float64 end . v = Vector{Float64}(undef, n) . struct LtvGQs active::Vector{Bool} end @model function btyd(custs::Array{CustomerData}) # Sub-model for churn active = Vector{Bool}(undef, length(custs)) future_purchases = Vector(undef, length(custs)) days_to_next_predicted_purchase = Vector(undef, length(custs)) for i in 1:length(custs) days_to_next_predicted_purchase[i] ~ Exponential(custs[i].frequency) active[i] = days_to_next_predicted_purchase[i] &gt; custs[i].recency # future_purchases[i] ~ Normal(active[i], .1) # 1/custs[i].frequency) end # Sub-model for survival analysis return active, active2 end . btyd (generic function with 1 method) . cust_data = [ CustomerData(2, 10.0, 123), CustomerData(10, 10, 123), CustomerData(23, 10, 123), CustomerData(2, 2, 123), CustomerData(10, 2, 123), CustomerData(23, 2, 123), ] . 6-element Array{CustomerData,1}: CustomerData(2, 10.0, 123.0) CustomerData(10, 10.0, 123.0) CustomerData(23, 10.0, 123.0) CustomerData(2, 2.0, 123.0) CustomerData(10, 2.0, 123.0) CustomerData(23, 2.0, 123.0) . iterations = 2000 ϵ = 0.05 τ = 10; chain_ltv = sample( btyd(cust_data), HMC(ϵ, τ), iterations, progress=true) . . Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00 . Chains MCMC chain (2000×15×1 Array{Float64,3}): Iterations = 1:2000 Thinning interval = 1 Chains = 1 Samples per chain = 2000 parameters = days_to_next_predicted_purchase[1], days_to_next_predicted_purchase[2], days_to_next_predicted_purchase[3], days_to_next_predicted_purchase[4], days_to_next_predicted_purchase[5], days_to_next_predicted_purchase[6] internals = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, n_steps, nom_step_size, step_size Summary Statistics parameters mean std naive_se mcse ⋯ Symbol Float64 Float64 Float64 Float64 ⋯ days_to_next_predicted_purchase[1] 9.1810 9.3432 0.2089 0.7770 ⋯ days_to_next_predicted_purchase[2] 9.8585 9.7331 0.2176 0.8114 ⋯ days_to_next_predicted_purchase[3] 10.6240 9.7998 0.2191 0.6761 ⋯ days_to_next_predicted_purchase[4] 1.7863 1.8346 0.0410 0.1468 ⋯ days_to_next_predicted_purchase[5] 1.9792 1.9113 0.0427 0.1309 ⋯ days_to_next_predicted_purchase[6] 1.8254 1.9007 0.0425 0.1877 ⋯ 2 columns omitted Quantiles parameters 2.5% 25.0% 50.0% 75.0% ⋯ Symbol Float64 Float64 Float64 Float64 ⋯ days_to_next_predicted_purchase[1] 0.1603 2.3690 6.4214 12.8901 ⋯ days_to_next_predicted_purchase[2] 0.2222 2.8749 6.8731 13.9526 ⋯ days_to_next_predicted_purchase[3] 0.5953 3.4112 7.8910 14.9332 ⋯ days_to_next_predicted_purchase[4] 0.0942 0.5068 1.1622 2.4613 ⋯ days_to_next_predicted_purchase[5] 0.0562 0.6334 1.4277 2.7009 ⋯ days_to_next_predicted_purchase[6] 0.0138 0.4309 1.2307 2.5427 ⋯ 1 column omitted . ltv_df = DataFrame(chain_ltv) . iterationchainacceptance_ratedays_to_next_predicted_purchasehamiltonian_energy . Int64Int64Float64Float64Float64 . 200 rows × 12 columns (omitted printing of 7 columns) . 11 | 1 | 1.0 | 1.37874 | 21.8509 | . 22 | 1 | 1.0 | 4.07641 | 15.0957 | . 33 | 1 | 1.0 | 5.71715 | 9.18893 | . 44 | 1 | 1.0 | 8.31854 | 7.91616 | . 55 | 1 | 0.999205 | 7.2199 | 7.3525 | . 66 | 1 | 1.0 | 11.8225 | 7.58637 | . 77 | 1 | 1.0 | 8.66177 | 7.22491 | . 88 | 1 | 1.0 | 9.47284 | 7.06928 | . 99 | 1 | 0.999398 | 8.01086 | 7.16072 | . 1010 | 1 | 0.992705 | 19.3259 | 9.22283 | . 1111 | 1 | 1.0 | 14.1054 | 9.56028 | . 1212 | 1 | 0.991303 | 4.7155 | 9.8923 | . 1313 | 1 | 1.0 | 6.40599 | 8.60139 | . 1414 | 1 | 0.998859 | 5.54836 | 8.13662 | . 1515 | 1 | 1.0 | 5.65508 | 8.32777 | . 1616 | 1 | 1.0 | 8.86628 | 7.97622 | . 1717 | 1 | 0.999718 | 12.0781 | 7.22928 | . 1818 | 1 | 1.0 | 9.03118 | 7.20782 | . 1919 | 1 | 0.999942 | 8.85645 | 7.06562 | . 2020 | 1 | 0.999693 | 8.19931 | 7.14217 | . 2121 | 1 | 0.983261 | 23.7124 | 10.7206 | . 2222 | 1 | 1.0 | 12.2768 | 10.9452 | . 2323 | 1 | 0.9906 | 19.0171 | 8.99959 | . 2424 | 1 | 1.0 | 13.1287 | 9.16857 | . 2525 | 1 | 0.998341 | 14.6708 | 7.832 | . 2626 | 1 | 0.995499 | 17.5793 | 9.06268 | . 2727 | 1 | 1.0 | 7.75418 | 8.7589 | . 2828 | 1 | 1.0 | 8.83522 | 7.21419 | . 2929 | 1 | 0.99838 | 6.70334 | 7.493 | . 3030 | 1 | 1.0 | 14.5174 | 8.37813 | . &vellip;&vellip; | &vellip; | &vellip; | &vellip; | &vellip; | . ltv_df. . chain_ltv . Chains MCMC chain (200×10×1 Array{Float64,3}): Iterations = 1:200 Thinning interval = 1 Chains = 1 Samples per chain = 200 parameters = days_to_next_predicted_purchase internals = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, n_steps, nom_step_size, step_size Summary Statistics parameters mean std naive_se mcse ⋯ Symbol Float64 Float64 Float64 Float64 ⋯ days_to_next_predicted_purchase 9.3565 3.9885 0.2820 0.2072 1 ⋯ 2 columns omitted Quantiles parameters 2.5% 25.0% 50.0% 75.0% ⋯ Symbol Float64 Float64 Float64 Float64 Fl ⋯ days_to_next_predicted_purchase 3.6326 6.6516 8.5672 11.4335 18 ⋯ 1 column omitted . plot(DataFrame(chain_ltv), x=:, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . . Plot(...) .",
            "url": "http://www.bwg.is/julia/turing/churn/survival/ltv/2021/01/05/Buy-Til-You_Die.html",
            "relUrl": "/julia/turing/churn/survival/ltv/2021/01/05/Buy-Til-You_Die.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Some More Models on Binary Data",
            "content": "To get myself comfortable with Julia and Turing.jl, I wrote out a bunch of toy models on the simplest of datasets: a simple vector of binary outcomes. To keep myself honest and to have something to refer to, I&#39;m reproducing them here. . using Turing using Bijectors using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(900px, 300px) . . Sequential Dependencies . The preceeding examples are about as far as coin flipping can take us, so let&#39;s begin exploring data that has more interesting structure. The first challenge we&#39;ll tackle is dependence between subsequent data elements. This is cheating a little bit, since we&#39;re no longer strictly dealing with binary data as we are also incorporating the order of the elements. In my opinion, this lives in the realm of prior knowledge: we believe a priori that the sort order of the data we received is equal to the temporal order of the data as it was generated. . Here is a very simple model to capture this dependency: . $$ begin{aligned} beta_0 &amp; sim mathrm{Beta}(1,1) beta_1 &amp; sim mathrm{Beta}(1,1) y_i &amp; sim begin{cases} mathrm{Bernoulli}( beta_0) &amp; y_{i-1} = 0 mathrm{Bernoulli}( beta_1) &amp; y_{i-1} = 1 end{cases} end{aligned}$$Note that if we incorrectly accepted an assumption of independence as in our previous models then our point estimate of p will not converge as quickly as expected, and our confidence intervals will be much too narrow. In other words, if we have positive autocorrelation (flipping a 1 is more likely if the preceeding flip was also a 1, and the reverse) then our estimates will just be wrong unless we account for it explicitly. . Let&#39;s generate some data in a way that very explicitly shows the connection to the previous datapoint: . data_autocor = [1] p_1 = 0.85 p_2 = 0.35 for i in 2:1000 if data_autocor[i-1] == 1 p = p_1 else p = p_2 end data_autocor = cat(data_autocor, rand(Bernoulli(p)), dims=1) end . And here&#39;s a model - it&#39;s not an accident that the model code looks so close to the data generating code! . @model function coinflip_autocor(y::Array{Int64}) # prior on p p_1 ~ Beta(1, 1) p_2 ~ Beta(1, 1) y[1] ~ Bernoulli(p_1) # I arbitrarily chose p_1 for i in 2:length(y) if y[i-1] == 1 p = p_1 else p = p_2 end y[i] ~ Bernoulli(p) end end; . Here are the estimates of the two parameters: . chain_autocor = sample(coinflip_autocor(data_autocor), HMC(ϵ, τ), iterations, progress=false)[500:end]; ch_autocor_df = @linq DataFrame(chain_autocor) |&gt; select(:p_1, :p_2) |&gt; DataFrames.stack(); plot(DataFrame(ch_autocor_df), x=:value, color=:variable, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0), Guide.yticks(label = false) ) . . value -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 p_1 p_2 variable h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? We have less certainty about $p_2$ than $p_1$. This is predictable from the data generation process: because heads are more likely to beget heads than tails are to beget tails (ie $p_1$ is closer to 1 than $p_2$ is to 0), we have more examples drawing from the $p_1$ distribution. . How to do this with an honest covariance matrix? ie over zip(y[:-1], y[2:])? What about a &quot;rich get richer&quot; scenario? P(1) ~ b + length of streak of 1s . would look good but how to diagnose issues? distributions of streaks! a lot of extremely long streaks, a lot of short streaks... | . | . what about a &quot;machine error&quot; - double readings for both or one of 1s / 0s? . Wide Dependencies: Rich get Richer . Look, we&#39;re long passed the physical analogy making sense for this data so just roll with it for now. The idea for this scenario is a minor extension of the former: every consecutive flip heads makes it more likely the next flip will also be heads, and the reverse for tails. . function streaks(arr) streaks = [] val = arr[1] current_streak = 1 for i in 2:length(arr) next_val = arr[i] if next_val == val current_streak = current_streak + 1 else streaks = cat(streaks, current_streak, dims=1) val = next_val current_streak = 1 end end streaks = cat(streaks, current_streak, dims=1) return streaks end; . plot(DataFrame(x=streaks(data_autocor)), x=:x, Geom.histogram) . x -40 -30 -20 -10 0 10 20 30 40 50 60 70 -30 -29 -28 -27 -26 -25 -24 -23 -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 -30 0 30 60 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100 110 120 130 -60 -58 -56 -54 -52 -50 -48 -46 -44 -42 -40 -38 -36 -34 -32 -30 -28 -26 -24 -22 -20 -18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114 116 118 120 -100 0 100 200 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 Many Streams with Discrete Biases . &quot;data on several coins from specific mints. Coin of unknown origin with 10 flips, what is P(heads) on next flip?&quot; . Many Streams with Continuous Biases . All coins come from one rather shitty mint. How shitty is that mint, and how shitty is this coin? . $$ begin{aligned} mu_{mint} &amp; sim mathrm{Normal}(0,1) sigma_{mint} &amp; sim mathrm{Normal}(1, 2) mu_{coin} &amp; sim mathrm{Normal}( mu_{mint}, sigma_{mint}) ys &amp; sim mathrm{Bernoulli}( mu_{coin}) end{aligned}$$Compare naive estimate and regularized estimate. Note: regularization lets us &quot;share error&quot; between . &quot;Seasonal&quot; data . e.g. hourly aggregate from a motion sensor in a shop: &quot;there was motion in the shop over the past hour&quot; . Extremely sparse data? .",
            "url": "http://www.bwg.is/julia/turing/binary/2021/01/03/Turing-Examples-2.html",
            "relUrl": "/julia/turing/binary/2021/01/03/Turing-Examples-2.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Some Simple Models on Binary Data",
            "content": "To get myself comfortable with Julia and Turing.jl, I wrote out a bunch of toy models on the simplest of datasets: a vector of binary outcomes. To keep myself honest and to have something to refer to, I&#39;m reproducing them here. . using Turing using Bijectors using Gadfly using DataFrames, DataFramesMeta Gadfly.set_default_plot_size(900px, 300px) . . Standard Biased Coin Estimation . Coinflipping is a time-honored introductory example for probability, and for good reason. Readily interpretable, amenable to both analytical solutions and manual calculation. Nevermind that &quot;unfair&quot; coins are essentially impossible to make... . We&#39;ll build two models that are essentially the same but have distinct computational profiles. Bernoulli: . $$ begin{aligned} beta &amp; sim mathrm{Beta}(1,1) y_i &amp; sim mathrm{Bernoulli}( beta) end{aligned}$$And Binomial: . $$ begin{aligned} beta &amp; sim mathrm{Beta}(1,1) mathrm{sum}(y) &amp; sim mathrm{Binomial}( mathrm{length}(y), beta) end{aligned}$$ Bernoulli Model . First thing to do is to translate the Bernoulli code above into a @model in Turing: . @model function coinflip(y) # prior on p p ~ Beta(1, 1) # updates on p for i in 1:length(y) y[i] ~ Bernoulli(p) end end; . It looks more or less like the description above, with the loop for i in 1:length(y) serving the same purpose as the subscript of $y_i$. . data = [1,1,1,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0] # Settings of the Hamiltonian Monte Carlo (HMC) sampler. iterations = 2000 ϵ = 0.05 τ = 10; chain_bernoulli = sample(coinflip(data), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_bernoulli), x=:p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 -5 0 5 10 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 Binomial Model . Now let&#39;s take a look at the equivalent model using a Binomial distribution that expresses the exact same underlying phenomena. In Turing: . @model function coinflip_binomial(heads::Int64, flips::Int64) # prior on p p ~ Beta(1, 1) # update on p heads ~ Binomial(flips, p) end; . heads = sum(data) flips = length(data) chain_binomial = sample( coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_binomial), x=:p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(xmin=0.0, xmax=1.0, ymin=0.0) ) . . p -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 -1 0 1 2 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 -5 0 5 10 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 Other Parameter Spaces . Going further afield, we can also model $ beta$ on an unconstrained scale ($ mathbb{R}$ instead of the unit interval $[0,1]$) by linking the domains with $ mathrm{logit}^{-1}(x) = frac{e^x}{e^x + 1}$. . $$ begin{aligned} rho &amp; sim mathrm{Beta}(1,1) beta &amp;= mathrm{logit}^{-1}( rho) y_i &amp; sim mathrm{Bernoulli}( beta) end{aligned}$$On it&#39;s own, this seems like a frivolous transformation but it allows us to incorporate multiple signals into our determination of the probability of a positive result. Turns out, this is just logistic regression. Imagine we have one outcome $y$ that depends on several inputs $x_1, x_2, ldots, x_n$: . $$ begin{aligned} rho_i &amp; sim mathrm{Normal}(0,1) beta &amp;= mathrm{logit}^{-1}( rho_1 x_1 + rho_2 x_2 + ldots + rho_n x_n) y_i &amp; sim mathrm{Bernoulli}( beta) end{aligned}$$Because we pass the regression product through $ mathrm{logit}^{-1}$, we know the value of $ beta$ will be between 0 and 1. If we didn&#39;t apply this transformation then adding up the components of $ beta$ could lead to values outside of that range, (and therefore not in the support of the Bernoulli distribution) which would cause the computation to fail. . The same approach works for the Binomial model: . $$ begin{aligned} rho_i &amp; sim mathrm{Normal}(0,1) beta &amp;= mathrm{logit}^{-1}( rho_1 x_1 + rho_2 x_2 + ldots + rho_n x_n) mathrm{sum}(y) &amp; sim mathrm{Binomial}( mathrm{length}(y), beta) end{aligned}$$Turing allows us to create a logit function in two ways. One is in pure Julia code (note I didn&#39;t apply a type assertion to the x argument: things can get messy here with automatic differentiation): . function invlogit(x) ex = exp(x) return ex / (1 + ex) end; . It&#39;s also possible to use some transformations that Turing uses internally from the Bijectors.jl library. Some sampling algorithms like HMC require an unbounded sampling space to (or something like it subject to Float64 constraints...). In order to enable that, the Bijectors library creates functions that map (smoothly) between continuous spaces. In . logit = bijector(Beta()) # bijection: (0, 1) → ℝ inv_logit = inv(logit) # bijection: ℝ → (0, 1) @model function coinflip_invlogit(heads::Int64, flips::Int64) # prior on p logit_p ~ Normal(0, 1) # or any unbounded distribution # update on p heads ~ Binomial(flips, invlogit(logit_p)) end; . heads = sum(data) flips = length(data) chain_invlogit = sample( coinflip_invlogit(heads, flips), HMC(ϵ, τ), iterations, progress=false); plot(DataFrame(chain_invlogit), x=:logit_p, Theme(alphas=[0.6]), Stat.density(bandwidth=0.02), Geom.polygon(fill=true, preserve_order=true), Coord.cartesian(ymin=0.0) ) . . logit_p -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 -6.0 -5.8 -5.6 -5.4 -5.2 -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 -6 -3 0 3 6 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 -2 0 2 4 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 Note the new x-axis which is the transformation of the previous axis. Here are some helpful values: . $$ begin{aligned} mathrm{logit}^{-1}(1.0) &amp;= 0.73 mathrm{logit}^{-1}(0) &amp;= frac{1}{2} mathrm{logit}^{-1}(-1.0) &amp;= 0.27 end{aligned}$$ Performance Difference . Despite essentially describing the same model, performance will differ significantly beteen the Binomial and the Bernoulli cases especially as the volume of data increases. The log-likelihood calculations in each sample loop are $O(1)$ versus $O( mathrm{rows})$, respectively. Here&#39;s a comparison with a modestly larger dataset: . long_data = repeat(data, 10); @time sample( coinflip(long_data), HMC(ϵ, τ), iterations, progress=false); . 7.658380 seconds (64.01 M allocations: 1.933 GiB, 6.88% gc time) . heads = sum(long_data) flips = length(long_data) @time sample( coinflip_binomial(heads, flips), HMC(ϵ, τ), iterations, progress=false); . 0.105582 seconds (659.91 k allocations: 46.448 MiB, 24.33% gc time) . Caution! . Just a little &quot;gotcha&quot; I ran into. The following model won&#39;t work, due to design choices in Turing: . @model function coinflip_BROKEN_THIS_WONT_WORK_AAAAAGH(y::Array{Int64}) # prior on p p ~ Beta(1, 1) # updates on p # these lines are the problem. heads = sum(y) # heads is not RV or argument heads ~ Binomial(length(y), p) # so it can&#39;t be on LHS of ~ end; . The reason is that Turing assumes that all variables on the left hand side of ~ are either . random variables (generated by Turing) or | declared as inputs to the function | By way of analogy to Stan, the @model block of Turing is only intended to represent the parameter, tranformed parameter and model blocks of Stan. As Turing is just Julia code, this is little inconvenience (just do the data transformations in Julia first, it&#39;s more efficient anyway!) but it can be a bit of a surprise and the error message is difficult to interpret. . Incorporating Prior Knowledge . The above models depend on unexplained constants in the Beta distribution ($ mathrm{Beta}(1,1)$ vs, say, $ mathrm{Beta}(13.859, pi^e)$). Those values are left unjustified and here we&#39;ll put some more thought into them. . Let&#39;s assume you are passingly familiar with the concept of flipping coins. Consequently you reasonably expect nearly every coin to be fair and nearly every person you know to not be Persi Diaconis. Personally, I&#39;d need to see a lot of data to accept that a coin had a substantial bias, so in this section we explore to bake that prior knowledge into our model as our prior distribution (the parameters of Beta, in this case). This is a small change to the model but we&#39;ll explore the impacts of a few alternatives for constraints on $ beta$: . $$ begin{aligned} beta &amp; sim mathrm{Beta}(1,1) &amp; mathrm{ versus} beta &amp; sim mathrm{Beta}(5,5) &amp; mathrm{ versus} beta &amp; sim mathrm{Beta}(50,50) &amp; mathrm{ versus} beta &amp; sim mathrm{Beta}(15,5)&amp; mathrm{ (for fun)} end{aligned}$$ plot distributions http://gadflyjl.org/stable/gallery/statistics/# . With 3 priors (Beta(1,1), Beta(4.9, 5.1), Beta(49, 51)): . Plot comparison of prior distributions | Plot comparison of posterior distributions |",
            "url": "http://www.bwg.is/julia/turing/binary/2020/12/30/Turing-Examples-1.html",
            "relUrl": "/julia/turing/binary/2020/12/30/Turing-Examples-1.html",
            "date": " • Dec 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "http://www.bwg.is/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://www.bwg.is/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}